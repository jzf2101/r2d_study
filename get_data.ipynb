{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import collect_data\n",
    "import re\n",
    "from importlib import reload\n",
    "from bs4 import BeautifulSoup\n",
    "from contextlib import suppress\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('colorblind')\n",
    "sns.set_color_codes()\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Data From the NIPS 2017 Schedule\n",
    "\n",
    "NIPS 2017 includes canonical links the authors' repository for a given paper.  We use BeautiulSoup to scrape the website's schedule metadata for paper information and the link to the repository.\n",
    "\n",
    "## To collect data, an internet connection and (optionally) a GitHub API key are necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get(\"https://nips.cc/Conferences/2017/Schedule\").content, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "poster_tables = soup.find_all('div', {'class':'maincard narrower Poster'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.DataFrame(index=range(len(poster_tables)),\n",
    "                      columns=['title', 'authors', 'paper_url', 'code_url', 'poster_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in papers.index:\n",
    "    title = poster_tables[p].find('div',{'class':'maincardBody'}).contents[0]\n",
    "    authors = poster_tables[p].find('div',{'class':'maincardFooter'}).contents[0]\n",
    "    pdf = collect_data.get_url(poster_tables[p], 'Paper')\n",
    "    repo = collect_data.get_url(poster_tables[p], 'Code')\n",
    "    poster_pdf = collect_data.get_url(poster_tables[p], 'Poster')\n",
    "    papers.loc[p] = pd.Series(dict(zip(papers.columns,\n",
    "                                       [title, authors, pdf, repo, poster_pdf])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_papers = papers[papers['code_url'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also test the urls for GitHub repos by making a request to the listed webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/forde/miniconda3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "code_papers['github'] = code_papers['code_url'].str.contains(\"github.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_papers['live'] = [collect_data.test_url(url) for url in code_papers['code_url']]\n",
    "code_papers['gh_full_path'] = np.logical_and(code_papers['github'], \n",
    "                                             code_papers['code_url'].str.rstrip('/')\\\n",
    "                                             .str.split('/').str.len() > 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some papers are not live at the time of testing, so we extract these and manually review to find the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_papers[np.logical_or(code_papers['live'] != True, ~code_papers['gh_full_path'])].to_csv('validate_url.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live_gh = code_papers[np.logical_and(code_papers['gh_full_path'], code_papers['live'] == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect scraped data on the existence of files in each repo that can be used by repo2docker by inspecting the GitHub page of each repo for the file.  The file types are listed in `file_types`.\n",
    "\n",
    "GitHub metadata is collected using the GitHub graphql api.  To run this query, one will need to create a [personal access token with GitHub](https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_types = ['Dockerfile', 'binder', 'apt.txt', 'environment.yml',\n",
    "          'requirements.txt', 'postBuild', 'setup.py', 'REQUIRE', 'runtime.txt',\n",
    "          'install.R']\n",
    "social_vals = ['description', 'stargazers', 'watchers',\n",
    "           'forks', 'languages', 'repositoryTopics'] \n",
    "r2d_checks = pd.DataFrame(index=live_gh['code_url'], columns=file_types)\n",
    "with suppress(Exception):\n",
    "    for i in r2d_checks.index:\n",
    "        gh_url = re.sub(r\"\\.git$\", \"\", i)\n",
    "        soup_i = BeautifulSoup(requests.get(gh_url).content, \"html5lib\")\n",
    "        r2d_checks.loc[i] = pd.Series(collect_data.request_inspect_for_r2d(i, soup_i, file_types))\n",
    "r2d_checks.to_csv('gh_r2d_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recollect data, you will need to replace the line below with your GitHub personal access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credentials = yaml.load(open('../binder_study/secrets.yml'))  \n",
    "    gh_metadata = pd.DataFrame(index=live_gh['code_url'], columns=social_vals)\n",
    "    with suppress(Exception):\n",
    "        for i in gh_metadata.index:\n",
    "            gh_url = re.sub(r\"\\.git$\", \"\", i)\n",
    "            split_url = gh_url.split('/')\n",
    "            gh_metadata.loc[i] = pd.Series(collect_data.graphql_social_data(split_url[3],\n",
    "                                    split_url[4], credentials['binder']['key']))\n",
    "    live_gh[live_gh['code_url'].isin(gh_metadata[gh_metadata.isna().all(axis=1)]\\\n",
    "                                     .index.values)].to_csv('change_reponame.csv')\n",
    "    gh_metadata = collect_data.process_gh_api_df(gh_metadata.dropna(how='all'))\n",
    "    gh_metadata[gh_metadata['no_code']].to_csv('no_code.csv')\n",
    "    gh_metadata = gh_metadata[~gh_metadata['no_code']] \n",
    "    gh_metadata.to_csv('gh_metadata.csv')\n",
    "except:\n",
    "    gh_metadata = pd.read_csv('gh_metadata.csv', index_col=0)                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of processing, we strip `.git` extensions to standardize our data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The block of code below collects all the data from GitHub. Again, this does not run except with a personal access token. Repo2docker file information from this block is saved as `gh_r2d_data.csv`, and GitHub metadata is saved as `gh_metadata.csv`.  Note also we exclude repositories that have changed their URL and include in later analysis.\n",
    "\n",
    "We have supressed these exceptions to get the notebook to run the whole way through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop data that does not have any programming languages associated with the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2d_checks = r2d_checks[r2d_checks.index.isin((~gh_metadata['no_code']).index)]\n",
    "r2d_checks = pd.read_csv('gh_r2d_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv's below are the results of the original data pull should you want to load the data from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Manually Annotated Data\n",
    "\n",
    "For repositories that do not have automatically accessible repositories available, we manually annotate to find the author's URL. Files with these lables are loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_labeled = pd.read_csv('validate_url_w_labels.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_url_labeled = pd.read_csv('change_reponame_labeled.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2d_labeled = pd.DataFrame(index=gh_labeled[gh_labeled['labeled_url'].str[:19] == 'https://github.com/']\n",
    "                           ['labeled_url'].append(changed_url_labeled['labeled_url']), columns=file_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_metadata_labeled = pd.DataFrame(index=r2d_labeled.index, columns=social_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now collect data for these labeled URLs using the same methodology to find configuration files and GitHub metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress(Exception):\n",
    "    for i in r2d_labeled.index:\n",
    "        split_url = i.split('/')\n",
    "        soup_i = BeautifulSoup(requests.get(i).content, \"html5lib\")\n",
    "        if i not in r2d_checks.index:\n",
    "            r2d_labeled.loc[i] = pd.Series(collect_data.request_inspect_for_r2d(i, soup_i, file_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for i in r2d_labeled.index:\n",
    "        gh_metadata_labeled.loc[i] = pd.Series(collect_data.graphql_social_data(split_url[3], split_url[4], credentials['binder']['key']))\n",
    "    gh_metadata_labeled = collect_data.process_gh_api_df(gh_metadata_labeled)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled and unlabeled datasets are combined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    total_gh_metadata = pd.concat([gh_metadata, gh_metadata_labeled])\n",
    "    total_r2d = pd.concat([r2d_checks, r2d_labeled])\n",
    "    total_gh_metadata.to_csv('gh_metadata_w_labeled.csv')\n",
    "    total_r2d.to_csv('r2d_w_labeled.csv')\n",
    "except:\n",
    "    total_gh_metadata = pd.read_csv('gh_metadata_w_labeled.csv', index_col=0)\n",
    "    total_r2d = pd.read_csv('r2d_w_labeled.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conference Wide Metrics\n",
    "\n",
    "We calculate the percent of papers that have included any code url, a paper url, and a url to a live GitHub page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_metrics = (len(papers) - papers.isnull().sum()).loc[['code_url','poster_url']]\\\n",
    ".append(live_gh[['gh_full_path']].sum())\n",
    "paper_metrics.index = ['Shared Code', 'Shared Poster', 'Live GitHub Link']\n",
    "(100.*paper_metrics/len(papers)).plot(kind='bar', cmap='coolwarm')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "plt.title('Percent of NIPS Papers by Repo Metric')\n",
    "plt.yticks(np.arange(0,55,5))\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('paper_metrics.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the raw counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the percentages in the above plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100.*paper_metrics/len(papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming languages that repo2docker supports are popular among NIPS authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100*total_gh_metadata[['Python', 'Julia', 'R']].sum()/len(total_gh_metadata)).plot(kind='bar', cmap='coolwarm')\n",
    "plt.title('Percent of GitHub Repositories by Language')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pct_languages.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the primary languages of the repositories. By far, the most popular language is Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_languages= total_gh_metadata['primary_language'].value_counts()\n",
    "primary_languages.rename(index={'Jupyter Notebook': 'Jupyter'}, inplace=True)\n",
    "primary_languages.plot(kind='bar', cmap='coolwarm')\n",
    "plt.title('Repository Primary Language')\n",
    "plt.ylabel('Number of Repositories')\n",
    "plt.xticks(rotation=45, horizontalalignment='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('primary_language.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, we also include information on papers that were manually labeled.  A minority of them required manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_url_stats = gh_labeled['labeled_url'].notnull().value_counts()\n",
    "labeled_url_stats.index = ['found url', 'url still missing']\n",
    "labeled_url_stats.loc['missing_https'] = gh_labeled['missing_https'].sum()\n",
    "labeled_url_stats.loc['changed_url'] = len(changed_url_labeled)\n",
    "labeled_url_stats.index = ['found url', 'url still missing', 'missing https', 'changed url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_url_stats.plot(kind='bar',  cmap='coolwarm')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('')\n",
    "plt.yticks(np.arange(0, 20, step=2))\n",
    "plt.title('NIPS Papers by Needing Manual Review')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('manual_review_metrics.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100.*paper_metrics/len(papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Reults Figures\n",
    "\n",
    "For reference there are 197 papers in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_gh_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first plot the number of repositories by each type of configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_r2d.sum().sort_values(ascending=False).plot(kind='bar', colormap='coolwarm')\n",
    "plt.title('GitHub Repositories by Type of Config File')\n",
    "plt.ylabel('')\n",
    "plt.xticks(rotation=45, horizontalalignment='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('repos_by_r2d_file.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the raw values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_r2d.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the number of repositories that have 0, 1, 2, or 3 configuration files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_r2d.sum(axis=1).value_counts().plot(kind='bar', colormap='coolwarm', rot=0)\n",
    "plt.xlabel('Number of Config Files')\n",
    "plt.ylabel('Number of Repositories')\n",
    "plt.title('Repo2Docker Config File Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('total_r2d_files_plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw values of the plot above are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_r2d.sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now label repositories that are repo2docker compatible as repositories that have at least 1 type of configuration file that repo2docker uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gh_metadata['r2d_capable'] = total_r2d.sum(axis=1) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average number of users that have forked, starred, or watched is significantly higher when we exclude repositories that are part of larger libraries.  We label larger libraries as libraries that point to a specific folder in the repo as the code, rather than the main repository URL.  These libraries are part of larger deep learning repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_gh_metadata['larger_library'] = total_gh_metadata.index.str.rstrip('/').str.split('/').str.len() > 5\n",
    "main_repo = total_gh_metadata[~total_gh_metadata['larger_library']]\n",
    "wo_larger_test = ttest_ind(main_repo[main_repo['r2d_capable']][['forks', 'stargazers', 'watchers']],\n",
    "          main_repo[~main_repo['r2d_capable']][['forks', 'stargazers', 'watchers']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo_larger_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The means of these two groups are listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_means = main_repo.groupby('r2d_capable')[['forks', 'stargazers', 'watchers']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the percent difference between repos with and without these configuration files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gh_means.loc[True] - gh_means.loc[False])/gh_means.loc[False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the means with errorbars when excluding larger libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2d_gh_groupby = total_gh_metadata[~total_gh_metadata['larger_library']].groupby(['r2d_capable'])[['forks', 'stargazers', 'watchers']]\n",
    "r2d_gh_groupby.mean().T.plot(kind='bar', yerr=.975*r2d_gh_groupby.sem().T)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('GitHub Users')\n",
    "plt.title('Mean GitHub Engagement (CI=0.95)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('engagement_excl_lrg.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the number of repositories in each category. `r2d_capable` means the repo has at least 1 config file used by repo2docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_repo.groupby('r2d_capable')['stargazers'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the larger libraries we've excluded from our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_libraries = total_gh_metadata[total_gh_metadata['larger_library']]\n",
    "larger_libraries.to_csv('larger_libraries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we also plot a boxplot to highlight these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=total_gh_metadata[['forks', 'stargazers', 'watchers']], fliersize=5, width=.5)\n",
    "plt.title('Boxplot of Number of Users by GitHub Metric')\n",
    "plt.tight_layout()\n",
    "plt.savefig('gh_metrics_boxplot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
